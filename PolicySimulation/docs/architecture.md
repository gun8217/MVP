# Architecture Overview

## 1. Design Principle

본 프로젝트의 아키텍처는 다음 세 가지 원칙을 중심으로 설계된다.

1. **정책 독립성**: 특정 법안이나 정부에 종속되지 않는 범용 구조
2. **해석 가능성**: 결과보다 과정과 행동 변화의 설명 가능성 중시
3. **확장성**: 정책 유형·행위자 수·보상 구조의 점진적 확장 가능

이 아키텍처는 강화학습을 "예측 모델"이 아닌 **정책 실험용 시뮬레이션 엔진**으로 사용한다.

---

## 2. High-Level Architecture

```text
┌──────────────────────┐
│   Policy Definition  │
│ (Environment Params) │
└──────────┬───────────┘
           │
           ▼
┌──────────────────────┐
│   Simulation Engine  │◄──────────────┐
│   (RL Environment)   │               │
└──────────┬───────────┘               │
           │                           │
           ▼                           │
┌──────────────────────┐        ┌────────────────────┐
│        Agent         │◄──────►│   Reward Model     │
│ (Adaptive Behavior)  │        │  (Utility & Cost)  │
└──────────┬───────────┘        └──────┬─────────────┘
           │                           │
           ▼                           ▼
┌──────────────────────┐        ┌────────────────────┐
│   Action & Outcome   │───────►│   Metrics & Logs   │
│ (Behavior Trajectory)│        │ (Policy Evaluation)│
└──────────────────────┘        └────────────────────┘
```

---

## 3. Policy Environment

정책은 강화학습 환경(Environment)으로 추상화된다.

### 3.1 Environment Parameters

- 규제 강도 (Regulation Level)
- 처벌 수준 (Penalty Severity)
- 단속 확률 (Enforcement Probability)
- 대안 제도 존재 여부 (Alternative Options)

이 파라미터들은 환경 상태(State) 또는 전역 설정으로 관리된다.

### 3.2 Environment Dynamics

- 정책 파라미터 변경 시 상태 전이 규칙 변화
- 단속·처벌 발생 확률적 반영
- 장기적 누적 비용 또는 위험 반영 가능

---

## 4. Agent Modeling

에이전트는 정책 환경 하에서 **자신의 기대 보상을 극대화**하려는 행위자로 정의된다.

### 4.1 Action Space

- 합법적 행동
- 우회 행동
- 고위험 행동
- 대체 전략 선택

### 4.2 Learning Behavior

- 에이전트는 환경과 반복 상호작용
- 정책 변화에 따라 전략 재학습
- 단기 최적 행동 vs 장기 전략 간 균형 탐색

초기 MVP에서는 단일 또는 소수 에이전트부터 시작한다.

---

## 5. Reward Design

보상 함수는 정책 효과 해석의 핵심 요소다.

### 5.1 Individual Reward

- 행위로부터 얻는 효용
- 비용 및 시간 소모
- 제재 발생 시 손실

### 5.2 Social Indicators (Observed)

- 위험 행동 비율
- 우회 전략 빈도
- 장기 누적 사회 비용 (지표화)

정책 평가는 **보상 함수 자체의 최적화 결과가 아닌**,
행동 분포 변화와 지표의 추이를 함께 관찰한다.

---

## 6. Simulation Flow

1. 정책 파라미터 설정
2. 환경 초기화
3. 에이전트 학습 및 상호작용
4. 행동 궤적 및 보상 기록
5. 정책 시나리오 변경 후 반복
6. 결과 비교 및 해석

---

## 7. Output & Interpretation

본 프로젝트의 출력은 단일 예측 값이 아니다.

- 행동 분포 변화
- 전략 전환 시점
- 정책 변화에 대한 민감도

결과는 **정책 효과의 가능 범위 탐색**으로 해석된다.

---

## 8. Ethical Boundary

- 실제 범죄 또는 개인 행동 예측에 사용되지 않음
- 정책 판단을 자동화하지 않음
- 결과는 실험적 시뮬레이션으로 제한

---

## 9. Extension Path

- 다중 에이전트 환경
- 정책 조합 시나리오
- 웹 기반 시각화 대시보드 연계
- 정책 도메인별 환경 템플릿 확장

본 아키텍처 문서는 프로젝트 진행에 따라 지속적으로 업데이트된다.
